{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Takes the input paragraph and splits it into a list of sentences\n",
    "from sentence_splitter import SentenceSplitter, split_text_into_sentences\n",
    " \n",
    "splitter = SentenceSplitter(language='en')\n",
    "\n",
    "#Create a list of sentences from a text file\n",
    "\n",
    "def create_list(text):\n",
    "    with open(text, 'r',encoding='utf-8') as file:\n",
    "        data = file.read().replace('\\n', '')\n",
    "        #print(data)\n",
    "        sentence_list = splitter.split(data)\n",
    "    return sentence_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datasets\n",
    "from transformers import LongformerTokenizerFast, LongformerForSequenceClassification, Trainer, TrainingArguments, LongformerConfig\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from tqdm import tqdm\n",
    "import wandb\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = LongformerConfig()\n",
    "tokenizer = LongformerTokenizerFast.from_pretrained('allenai/longformer-base-4096', max_length = 1024)\n",
    "config = LongformerConfig.from_json_file('E:/Research/Plagiarism/Longformer_Epoch-1/config.json')\n",
    "model = LongformerForSequenceClassification.from_pretrained('E:/Research/Plagiarism/Longformer_Epoch-1/pytorch_model.bin',config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_prediction(sentence_list):\n",
    "    final_output = []\n",
    "    for i in range(20):\n",
    "        tokenized_sentence = tokenizer.encode(sentence_list[i])\n",
    "        input_ids = torch.tensor([tokenized_sentence])\n",
    "        with torch.no_grad():\n",
    "            output = model(input_ids)\n",
    "        label_indices = np.argmax(output[0].to('cpu').numpy(), axis=1)\n",
    "        print(int(label_indices))\n",
    "        final_output.append(int(label_indices))\n",
    "    count_1 = final_output.count(1)\n",
    "    count_0 = final_output.count(0)\n",
    "    pct_plagiarism = (count_1/len(final_output))*100\n",
    "    \n",
    "    return str(pct_plagiarism)+'%'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
